import os
import argparse
from typing import List, Dict, TypedDict, Annotated, Sequence, Any
from dotenv import load_dotenv

from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.agent_toolkits.load_tools import load_tools
from langchain_core.tools import BaseTool

import langgraph
from langgraph.graph import END, StateGraph


# Define the state of our graph
class AgentState(TypedDict):
    messages: List[Any]  # Can be HumanMessage, AIMessage, etc.
    tools: List[BaseTool]
    next: str


def create_agent(
    llm,
    tools: List[BaseTool],
    system_message: str = "You are a helpful AI assistant. Use tools when necessary.",
):
    """Create a single agent node in our graph with the given tools."""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_message),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )

    agent = create_tool_calling_agent(llm, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools)

    # Create a wrapper function to maintain the expected interface
    def agent_executor(state):
        input_messages = state["messages"]
        response = executor.invoke({"messages": input_messages})
        return {"messages": input_messages + [AIMessage(content=response["output"])]}

    return agent_executor


# Define graph state handlers
def route_based_on_task(state: AgentState) -> Dict:
    """Route to the appropriate agent based on the last message."""
    last_message = state["messages"][-1]
    if not isinstance(last_message, HumanMessage):
        state["next"] = "general_agent"
        return state

    query = last_message.content.lower()

    if any(keyword in query for keyword in ["math", "calculate", "compute"]):
        state["next"] = "math_agent"
    elif any(
        keyword in query
        for keyword in ["research", "information", "who", "what", "when"]
    ):
        state["next"] = "research_agent"
    else:
        state["next"] = "general_agent"
    
    return state


def build_graph(
    model_name: str = "gemini-2.0-flash",
    *,
    temperature: float = 0.2,
):
    """Build a LangGraph with specialized agent nodes for different tasks."""
    load_dotenv()

    if not os.environ.get("GOOGLE_API_KEY"):
        raise EnvironmentError("GOOGLE_API_KEY environment variable is not set.")

    # Initialize LLM
    llm = ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature,
    )

    # Create tools for different agents
    math_tools = load_tools(["llm-math"], llm=llm)
    research_tools = load_tools(["wikipedia"], llm=llm)
    general_tools = load_tools(["llm-math", "wikipedia"], llm=llm)

    # Create the graph with the initial state schema
    workflow = StateGraph(AgentState)

    # Define our agent nodes
    math_agent = create_agent(
        llm=llm,
        tools=math_tools,
        system_message="You are a mathematical expert. Focus on solving calculations accurately.",
    )

    research_agent = create_agent(
        llm=llm,
        tools=research_tools,
        system_message="You are a research assistant. Focus on providing accurate information.",
    )

    general_agent = create_agent(
        llm=llm,
        tools=general_tools,
        system_message="You are a helpful assistant that can handle a variety of tasks.",
    )

    # Add nodes to the graph
    workflow.add_node("math_agent", math_agent)
    workflow.add_node("research_agent", research_agent)
    workflow.add_node("general_agent", general_agent)
    workflow.add_node("router", route_based_on_task)

    # Define conditional edges based on the "next" field in state
    workflow.add_conditional_edges(
        "router",
        lambda state: state["next"],
        {
            "math_agent": "math_agent",
            "research_agent": "research_agent",
            "general_agent": "general_agent"
        }
    )

    # Connect agent outputs back to END
    workflow.add_edge("math_agent", END)
    workflow.add_edge("research_agent", END)
    workflow.add_edge("general_agent", END)

    # Set the entrypoint
    workflow.set_entry_point("router")

    # Compile the graph
    app = workflow.compile()
    return app


def main():
    """Run an interactive session with the LangGraph agent."""
    parser = argparse.ArgumentParser(description="LangGraph Workflow Example")
    parser.add_argument(
        "--model", default="gemini-2.0-flash", help="Google Generative AI model to use"
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose mode")
    args = parser.parse_args()

    graph = build_graph(model_name=args.model)

    print("=== LangGraph Workflow Demo ===")
    print(
        "This demo shows a graph-based agent that routes queries to specialized agents:"
    )
    print("- Math Agent: Handles calculations and mathematical questions")
    print("- Research Agent: Retrieves information from Wikipedia")
    print("- General Agent: Handles everything else with all available tools")
    print("\nType 'exit' to quit.")

    # Initialize state
    state = {"messages": [], "tools": [], "next": ""}

    while True:
        try:
            user_input = input("\nUser> ")
        except (EOFError, KeyboardInterrupt):
            print()  # newline on Ctrl-C / Ctrl-D
            break

        if user_input.strip().lower() in {"exit", "quit", "q"}:
            break

        # Add the new user message to the state
        state["messages"].append(HumanMessage(content=user_input))

        try:
            # Process through the graph
            result = graph.invoke(state)

            # Extract the AI's response
            ai_message = result["messages"][-1]
            print(f"Agent> {ai_message.content}")

            # Update state for next iteration
            state = result

        except Exception as err:
            print(f"[⚠️] Workflow error: {err}")
            continue


if __name__ == "__main__":
    main()
